{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = pd.read_csv('train_transaction.csv')\n",
    "df_transaction_test = pd.read_csv(\"test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 394)\n",
      "(506691, 393)\n"
     ]
    }
   ],
   "source": [
    "print(df_transaction.shape)\n",
    "print(df_transaction_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isfraud = train.isFraud\n",
    "train = train.drop([\"TransactionID_x\",\"TransactionID_x\",\"isFraud\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"isFraud\"] = isfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop([\"TransactionID_x\",\"TransactionID_x\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>V40</th>\n",
       "      <th>V41</th>\n",
       "      <th>V42</th>\n",
       "      <th>V43</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V46</th>\n",
       "      <th>V47</th>\n",
       "      <th>V48</th>\n",
       "      <th>V49</th>\n",
       "      <th>V50</th>\n",
       "      <th>V51</th>\n",
       "      <th>V52</th>\n",
       "      <th>V53</th>\n",
       "      <th>V54</th>\n",
       "      <th>V55</th>\n",
       "      <th>V56</th>\n",
       "      <th>V57</th>\n",
       "      <th>V58</th>\n",
       "      <th>V59</th>\n",
       "      <th>V60</th>\n",
       "      <th>V61</th>\n",
       "      <th>V62</th>\n",
       "      <th>V63</th>\n",
       "      <th>V64</th>\n",
       "      <th>V65</th>\n",
       "      <th>V66</th>\n",
       "      <th>V67</th>\n",
       "      <th>V68</th>\n",
       "      <th>V69</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "      <th>V80</th>\n",
       "      <th>V81</th>\n",
       "      <th>V82</th>\n",
       "      <th>V83</th>\n",
       "      <th>V84</th>\n",
       "      <th>V85</th>\n",
       "      <th>V86</th>\n",
       "      <th>V87</th>\n",
       "      <th>V88</th>\n",
       "      <th>V89</th>\n",
       "      <th>V90</th>\n",
       "      <th>V91</th>\n",
       "      <th>V92</th>\n",
       "      <th>V93</th>\n",
       "      <th>V94</th>\n",
       "      <th>V95</th>\n",
       "      <th>V96</th>\n",
       "      <th>V97</th>\n",
       "      <th>V98</th>\n",
       "      <th>V99</th>\n",
       "      <th>V100</th>\n",
       "      <th>V101</th>\n",
       "      <th>V102</th>\n",
       "      <th>V103</th>\n",
       "      <th>V104</th>\n",
       "      <th>V105</th>\n",
       "      <th>V106</th>\n",
       "      <th>V107</th>\n",
       "      <th>V108</th>\n",
       "      <th>V109</th>\n",
       "      <th>V110</th>\n",
       "      <th>V111</th>\n",
       "      <th>V112</th>\n",
       "      <th>V113</th>\n",
       "      <th>V114</th>\n",
       "      <th>V115</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V118</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>V121</th>\n",
       "      <th>V122</th>\n",
       "      <th>V123</th>\n",
       "      <th>V124</th>\n",
       "      <th>V125</th>\n",
       "      <th>V126</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V129</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V132</th>\n",
       "      <th>V133</th>\n",
       "      <th>V134</th>\n",
       "      <th>V135</th>\n",
       "      <th>V136</th>\n",
       "      <th>V137</th>\n",
       "      <th>V138</th>\n",
       "      <th>V139</th>\n",
       "      <th>V140</th>\n",
       "      <th>V141</th>\n",
       "      <th>V142</th>\n",
       "      <th>V143</th>\n",
       "      <th>V144</th>\n",
       "      <th>V145</th>\n",
       "      <th>V146</th>\n",
       "      <th>V147</th>\n",
       "      <th>V148</th>\n",
       "      <th>V149</th>\n",
       "      <th>V150</th>\n",
       "      <th>V151</th>\n",
       "      <th>V152</th>\n",
       "      <th>V153</th>\n",
       "      <th>V154</th>\n",
       "      <th>V155</th>\n",
       "      <th>V156</th>\n",
       "      <th>V157</th>\n",
       "      <th>V158</th>\n",
       "      <th>V159</th>\n",
       "      <th>V160</th>\n",
       "      <th>V161</th>\n",
       "      <th>V162</th>\n",
       "      <th>V163</th>\n",
       "      <th>V164</th>\n",
       "      <th>V165</th>\n",
       "      <th>V166</th>\n",
       "      <th>V167</th>\n",
       "      <th>V168</th>\n",
       "      <th>V169</th>\n",
       "      <th>V170</th>\n",
       "      <th>V171</th>\n",
       "      <th>V172</th>\n",
       "      <th>V173</th>\n",
       "      <th>V174</th>\n",
       "      <th>V175</th>\n",
       "      <th>V176</th>\n",
       "      <th>V177</th>\n",
       "      <th>V178</th>\n",
       "      <th>V179</th>\n",
       "      <th>V180</th>\n",
       "      <th>V181</th>\n",
       "      <th>V182</th>\n",
       "      <th>V183</th>\n",
       "      <th>V184</th>\n",
       "      <th>V185</th>\n",
       "      <th>V186</th>\n",
       "      <th>V187</th>\n",
       "      <th>V188</th>\n",
       "      <th>V189</th>\n",
       "      <th>V190</th>\n",
       "      <th>V191</th>\n",
       "      <th>V192</th>\n",
       "      <th>V193</th>\n",
       "      <th>V194</th>\n",
       "      <th>V195</th>\n",
       "      <th>V196</th>\n",
       "      <th>V197</th>\n",
       "      <th>V198</th>\n",
       "      <th>V199</th>\n",
       "      <th>V200</th>\n",
       "      <th>V201</th>\n",
       "      <th>V202</th>\n",
       "      <th>V203</th>\n",
       "      <th>V204</th>\n",
       "      <th>V205</th>\n",
       "      <th>V206</th>\n",
       "      <th>V207</th>\n",
       "      <th>V208</th>\n",
       "      <th>V209</th>\n",
       "      <th>V210</th>\n",
       "      <th>V211</th>\n",
       "      <th>V212</th>\n",
       "      <th>V213</th>\n",
       "      <th>V214</th>\n",
       "      <th>V215</th>\n",
       "      <th>V216</th>\n",
       "      <th>V217</th>\n",
       "      <th>V218</th>\n",
       "      <th>V219</th>\n",
       "      <th>V220</th>\n",
       "      <th>V221</th>\n",
       "      <th>V222</th>\n",
       "      <th>V223</th>\n",
       "      <th>V224</th>\n",
       "      <th>V225</th>\n",
       "      <th>V226</th>\n",
       "      <th>V227</th>\n",
       "      <th>V228</th>\n",
       "      <th>V229</th>\n",
       "      <th>V230</th>\n",
       "      <th>V231</th>\n",
       "      <th>V232</th>\n",
       "      <th>V233</th>\n",
       "      <th>V234</th>\n",
       "      <th>V235</th>\n",
       "      <th>V236</th>\n",
       "      <th>V237</th>\n",
       "      <th>V238</th>\n",
       "      <th>V239</th>\n",
       "      <th>V240</th>\n",
       "      <th>V241</th>\n",
       "      <th>V242</th>\n",
       "      <th>V243</th>\n",
       "      <th>V244</th>\n",
       "      <th>V245</th>\n",
       "      <th>V246</th>\n",
       "      <th>V247</th>\n",
       "      <th>V248</th>\n",
       "      <th>V249</th>\n",
       "      <th>V250</th>\n",
       "      <th>V251</th>\n",
       "      <th>V252</th>\n",
       "      <th>V253</th>\n",
       "      <th>V254</th>\n",
       "      <th>V255</th>\n",
       "      <th>V256</th>\n",
       "      <th>V257</th>\n",
       "      <th>V258</th>\n",
       "      <th>V259</th>\n",
       "      <th>V260</th>\n",
       "      <th>V261</th>\n",
       "      <th>V262</th>\n",
       "      <th>V263</th>\n",
       "      <th>V264</th>\n",
       "      <th>V265</th>\n",
       "      <th>V266</th>\n",
       "      <th>V267</th>\n",
       "      <th>V268</th>\n",
       "      <th>V269</th>\n",
       "      <th>V270</th>\n",
       "      <th>V271</th>\n",
       "      <th>V272</th>\n",
       "      <th>V273</th>\n",
       "      <th>V274</th>\n",
       "      <th>V275</th>\n",
       "      <th>V276</th>\n",
       "      <th>V277</th>\n",
       "      <th>V278</th>\n",
       "      <th>V279</th>\n",
       "      <th>V280</th>\n",
       "      <th>V281</th>\n",
       "      <th>V282</th>\n",
       "      <th>V283</th>\n",
       "      <th>V284</th>\n",
       "      <th>V285</th>\n",
       "      <th>V286</th>\n",
       "      <th>V287</th>\n",
       "      <th>V288</th>\n",
       "      <th>V289</th>\n",
       "      <th>V290</th>\n",
       "      <th>V291</th>\n",
       "      <th>V292</th>\n",
       "      <th>V293</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V297</th>\n",
       "      <th>V298</th>\n",
       "      <th>V299</th>\n",
       "      <th>V300</th>\n",
       "      <th>V301</th>\n",
       "      <th>V302</th>\n",
       "      <th>V303</th>\n",
       "      <th>V304</th>\n",
       "      <th>V305</th>\n",
       "      <th>V306</th>\n",
       "      <th>V307</th>\n",
       "      <th>V308</th>\n",
       "      <th>V309</th>\n",
       "      <th>V310</th>\n",
       "      <th>V311</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V316</th>\n",
       "      <th>V317</th>\n",
       "      <th>V318</th>\n",
       "      <th>V319</th>\n",
       "      <th>V320</th>\n",
       "      <th>V321</th>\n",
       "      <th>V322</th>\n",
       "      <th>V323</th>\n",
       "      <th>V324</th>\n",
       "      <th>V325</th>\n",
       "      <th>V326</th>\n",
       "      <th>V327</th>\n",
       "      <th>V328</th>\n",
       "      <th>V329</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>M2</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "\n",
       "   card2  card3       card4  card5   card6  addr1  addr2  dist1  dist2  \\\n",
       "0    NaN  150.0    discover  142.0  credit  315.0   87.0   19.0    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  credit  325.0   87.0    NaN    NaN   \n",
       "\n",
       "  P_emaildomain R_emaildomain   C1   C2   C3   C4   C5   C6   C7   C8   C9  \\\n",
       "0           NaN           NaN  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0   \n",
       "1     gmail.com           NaN  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "\n",
       "   C10  C11  C12  C13  C14    D1  D2    D3   D4  D5  D6  D7  D8  D9   D10  \\\n",
       "0  0.0  2.0  0.0  1.0  1.0  14.0 NaN  13.0  NaN NaN NaN NaN NaN NaN  13.0   \n",
       "1  0.0  1.0  0.0  1.0  1.0   0.0 NaN   NaN  0.0 NaN NaN NaN NaN NaN   0.0   \n",
       "\n",
       "    D11  D12  D13  D14  D15   M1   M2   M3  M4 M5 M6   M7   M8   M9   V1   V2  \\\n",
       "0  13.0  NaN  NaN  NaN  0.0    T    T    T  M2  F  T  NaN  NaN  NaN  1.0  1.0   \n",
       "1   NaN  NaN  NaN  NaN  0.0  NaN  NaN  NaN  M0  T  T  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "    V3   V4   V5   V6   V7   V8   V9  V10  V11  V12  V13  V14  V15  V16  V17  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "\n",
       "   V18  V19  V20  V21  V22  V23  V24  V25  V26  V27  V28  V29  V30  V31  V32  \\\n",
       "0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   V33  V34  V35  V36  V37  V38  V39  V40  V41  V42  V43  V44  V45  V46  V47  \\\n",
       "0  0.0  0.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0   \n",
       "\n",
       "   V48  V49  V50  V51  V52  V53  V54  V55  V56  V57  V58  V59  V60  V61  V62  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
       "\n",
       "   V63  V64  V65  V66  V67  V68  V69  V70  V71  V72  V73  V74  V75  V76  V77  \\\n",
       "0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0   \n",
       "1  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "\n",
       "   V78  V79  V80  V81  V82  V83  V84  V85  V86  V87  V88  V89  V90  V91  V92  \\\n",
       "0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0   \n",
       "1  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   V93  V94  V95  V96  V97  V98  V99  V100  V101  V102  V103  V104  V105  \\\n",
       "0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0   0.0   1.0   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V106  V107  V108  V109  V110  V111  V112  V113  V114  V115  V116  V117  \\\n",
       "0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "1   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "\n",
       "   V118  V119  V120  V121  V122  V123  V124  V125  V126   V127  V128  V129  \\\n",
       "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0  117.0   0.0   0.0   \n",
       "1   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0    0.0   0.0   0.0   \n",
       "\n",
       "   V130  V131  V132   V133  V134  V135  V136  V137  V138  V139  V140  V141  \\\n",
       "0   0.0   0.0   0.0  117.0   0.0   0.0   0.0   0.0   NaN   NaN   NaN   NaN   \n",
       "1   0.0   0.0   0.0    0.0   0.0   0.0   0.0   0.0   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V142  V143  V144  V145  V146  V147  V148  V149  V150  V151  V152  V153  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V154  V155  V156  V157  V158  V159  V160  V161  V162  V163  V164  V165  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V166  V167  V168  V169  V170  V171  V172  V173  V174  V175  V176  V177  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V178  V179  V180  V181  V182  V183  V184  V185  V186  V187  V188  V189  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V190  V191  V192  V193  V194  V195  V196  V197  V198  V199  V200  V201  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V202  V203  V204  V205  V206  V207  V208  V209  V210  V211  V212  V213  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V214  V215  V216  V217  V218  V219  V220  V221  V222  V223  V224  V225  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V226  V227  V228  V229  V230  V231  V232  V233  V234  V235  V236  V237  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V238  V239  V240  V241  V242  V243  V244  V245  V246  V247  V248  V249  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V250  V251  V252  V253  V254  V255  V256  V257  V258  V259  V260  V261  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V262  V263  V264  V265  V266  V267  V268  V269  V270  V271  V272  V273  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V274  V275  V276  V277  V278  V279  V280  V281  V282  V283  V284  V285  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   0.0   0.0   0.0   1.0   1.0   0.0   0.0   \n",
       "1   NaN   NaN   NaN   NaN   NaN   0.0   0.0   0.0   1.0   1.0   0.0   0.0   \n",
       "\n",
       "   V286  V287  V288  V289  V290  V291  V292  V293  V294  V295  V296  V297  \\\n",
       "0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   0.0   1.0   0.0   0.0   0.0   \n",
       "1   0.0   0.0   0.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V298  V299  V300  V301  V302  V303  V304  V305  V306   V307  V308  V309  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  117.0   0.0   0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0    0.0   0.0   0.0   \n",
       "\n",
       "   V310  V311  V312  V313  V314  V315  V316   V317  V318  V319  V320  V321  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  117.0   0.0   0.0   0.0   0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V322  V323  V324  V325  V326  V327  V328  V329  V330  V331  V332  V333  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V334  V335  V336  V337  V338  V339  \n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = None\n",
    "df_transaction.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cleaning(object):\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.all_na = df.isnull().sum()/df.shape[0]\n",
    "\n",
    "    def clean_na(self,df):\n",
    "        self.less_na = all_na[all_na < 0.90]\n",
    "        return df.loc[:,less_na]\n",
    "    \n",
    "    def subset(self,df,index):\n",
    "        return df.iloc[:,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only small percentage of values are fraudalent.\n",
    "(df_transaction.isFraud.value_counts() / df_transaction.shape[0] ) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not removing columns simply because it has more NA\n",
    "#maybe more values in less number of rows can help us detect fraud\n",
    "#lets compare NAs in both tables fraud=0 and fraud=1\n",
    "df_isfraud_yes = df_transaction[df_transaction.isFraud==1]\n",
    "df_isfraud_no = df_transaction[df_transaction.isFraud==0]\n",
    "\n",
    "comparision = cleaning(df_isfraud_no)\n",
    "values_no = comparision.all_na.values\n",
    "comparision = cleaning(df_isfraud_yes)\n",
    "values_yes = comparision.all_na.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I computed percentage of na values in both fradualant and non fraudalent dataframe\n",
    "# this procudeure was to identify if let's say whole columns had 90% missing values\n",
    "# but rest 10% belongs to fraudalent dataset. That is why this was important to analyse indipendently\n",
    "pd.set_option('display.max_rows', 500)\n",
    "values = pd.DataFrame(values_no,values_yes).reset_index(inplace=False)\n",
    "values.columns = [\"fraud_no\",\"fraud_yes\"]\n",
    "values[\"difference\"] = ((values.fraud_no - values.fraud_yes) / values.fraud_no ) * 100 \n",
    "values.difference[values.difference.isna()] = 0\n",
    "values = values[(values.difference >= 0) & (values.fraud_no < 0.50) & (values.fraud_yes < 0.50)]\n",
    "index_values = values.index\n",
    "index_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I am calling the function subset in thr cleaning class\n",
    "comparision = cleaning(df_transaction)\n",
    "df_transaction = comparision.subset(df_transaction,index_values)\n",
    "df_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times=[]\n",
    "\n",
    "def time_conversion(df):\n",
    "    for i in range(df.shape[0]):\n",
    "        t = df.TransactionDT[i]\n",
    "        t = datetime.datetime.fromtimestamp( t )\n",
    "        t = t.replace()\n",
    "        times.append(t)\n",
    "    df.TransactionDT = times\n",
    "    return df\n",
    "\n",
    "df_transaction = time_conversion(df_transaction)\n",
    "df_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.Series( (round((df_transaction.TransactionDT /(60*60)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = pd.Series(df_transaction.isFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_anomiles = pd.concat([time,anomalies],axis=1)\n",
    "plt.plot(time_anomiles.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_anomiles = pd.concat([time,anomalies],axis=1)\n",
    "time_anomiles = time_anomiles.groupby(\"TransactionDT\").sum().reset_index()\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(time_anomiles.TransactionDT,time_anomiles.isFraud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_anomiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t = 86458\n",
    "t = (datetime.datetime.fromtimestamp(t))\n",
    "t = t.replace()\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "class predictions(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def data_munging(df):\n",
    "        return\n",
    "    def visulizations(df):\n",
    "        return\n",
    "    \n",
    "    def encoding(self,df):\n",
    "        self.obj = [columns for columns in df.columns if df[columns].dtypes==\"object\"]\n",
    "        le = LabelEncoder()\n",
    "        for col in self.obj:\n",
    "            df[col] = le.fit_transform(df[col].astype(\"str\").values)\n",
    "        return df\n",
    "    \n",
    "    def feature_selection(self,df,num_features):\n",
    "        df = df.fillna(0)\n",
    "        best = SelectKBest(chi2,num_features)\n",
    "        fit = best.fit_transform(df.iloc[:,:-1],df.iloc[:,-1])\n",
    "        fit = pd.DataFrame(fit)\n",
    "        return fit\n",
    "    \n",
    "    def normalize(self,df):\n",
    "        df_fraud = df[df.isFraud==1]\n",
    "        df_non_fraud = df[df.isFraud!=1]\n",
    "        rand = random.sample(range(0,df_non_fraud.shape[0]),df_fraud.shape[0] - 2000 )\n",
    "        df_non_fraud = df_non_fraud.iloc[rand]\n",
    "        return pd.concat([df_non_fraud,df_fraud])\n",
    "    \n",
    "    def boosting(self,df,classes,df_test):\n",
    "        df = df.fillna(0)\n",
    "        df_test = df_test.fillna(0)\n",
    "        \n",
    "        train_x,test_x , train_y,test_y = train_test_split(df,classes, test_size=0.25, random_state=36)\n",
    "        #smt = SMOTE()\n",
    "        #train_x,train_y = smt.fit_sample(train_x,train_y)\n",
    "        #train_x = pd.DataFrame(train_x)\n",
    "        #train_x.columns = test_x.columns\n",
    "        model = xgb.XGBClassifier(learning_rate =0.1,\n",
    "                 n_estimators=1500,\n",
    "                 max_depth=6,\n",
    "                 min_child_weight=3,\n",
    "                 gamma=0,\n",
    "                 subsample=0.8,\n",
    "                 colsample_bytree=0.8,\n",
    "                                  reg_alpha=0.005,\n",
    "                 objective= 'binary:logistic',\n",
    "                 nthread=4,\n",
    "                 scale_pos_weight=1,\n",
    "                 seed=27)\n",
    "                \n",
    "        model.fit(train_x,train_y)\n",
    "        pred_prob = model.predict_proba(test_x)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred = model.predict(test_x)\n",
    "        acc = accuracy_score(test_y,pred)\n",
    "        \n",
    "        auc = roc_auc_score(test_y,pred)\n",
    "        fpr, tpr, therashold = roc_curve(test_y,pred_prob)\n",
    "        report = classification_report(pred,test_y)\n",
    "        con_mat = confusion_matrix(test_y,pred)\n",
    "        \n",
    "        final_result = model.predict_proba(df_test)\n",
    "                \n",
    "        return auc, fpr,tpr,therashold, report, acc, con_mat, final_result\n",
    "        \n",
    "    \n",
    "    def logistic(self,df,classes):\n",
    "        df = df.fillna(0)\n",
    "        train_x,test_x , train_y,test_y = train_test_split(df,classes,random_state=57,test_size=0.25)\n",
    "        smt = SMOTE()\n",
    "        train_x,train_y = smt.fit_sample(train_x,train_y)\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(train_x,train_y)\n",
    "        pred_prob = model.predict_proba(test_x)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred = model.predict(test_x)\n",
    "        acc = accuracy_score(test_y,pred)\n",
    "        \n",
    "        auc = roc_auc_score(test_y,pred)\n",
    "        fpr, tpr, therashold = roc_curve(test_y,pred_prob)\n",
    "        report = classification_report(pred,test_y)\n",
    "        con_mat = confusion_matrix(test_y,pred)\n",
    "        return auc, fpr,tpr,therashold, report, acc, con_mat\n",
    "\n",
    "    \n",
    "    def randomforest(self,df,classes):\n",
    "        df = df.fillna(0)\n",
    "        train_x,test_x , train_y,test_y = train_test_split(df,classes,random_state=57,test_size=0.3)\n",
    "        model = RandomForestClassifier(n_estimators=50,max_depth=3,random_state=0)\n",
    "        model.fit(train_x,train_y)\n",
    "        pred_prob = model.predict_proba(test_x)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred = model.predict(test_x)\n",
    "        acc = accuracy_score(test_y,pred)\n",
    "        \n",
    "        auc = roc_auc_score(test_y,pred)\n",
    "        fpr, tpr, therashold = roc_curve(test_y,pred_prob)\n",
    "        report = classification_report(pred,test_y)\n",
    "        con_mat = confusion_matrix(test_y,pred)\n",
    "        \n",
    "        return auc, fpr,tpr,therashold, report, acc, con_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ksiro\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "step number 0, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 15376.0\n",
      "step number 10, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 20, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 30, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 40, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 50, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 60, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 70, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 80, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 90, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 100, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 110, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 120, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 130, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 140, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 150, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 160, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 170, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 180, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 190, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 200, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 210, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 220, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 230, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 240, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 250, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 260, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 270, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 280, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 290, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 300, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 310, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 320, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 330, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 340, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 350, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 360, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 370, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 380, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 390, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 400, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 410, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 420, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 430, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 440, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 450, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 460, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 470, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 480, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n",
      "step number 490, accuracy is 0.9652837514877319, cost is 15376.0, change in cost 0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final accuracy is {}\".format(sess.run(accuracy,feed_dict={X:test_x,Y_class:test_y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under the curve0.8888381335105893\n",
      "total accuracy0.8887921022067363\n",
      "confusion matrix is \n",
      " [[4613  526]\n",
      " [ 623 4570]] \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5236\n",
      "           1       0.88      0.90      0.89      5096\n",
      "\n",
      "    accuracy                           0.89     10332\n",
      "   macro avg       0.89      0.89      0.89     10332\n",
      "weighted avg       0.89      0.89      0.89     10332\n",
      "\n",
      "[[0.94035417 0.05964583]\n",
      " [0.9736614  0.02633858]\n",
      " [0.9700917  0.0299083 ]\n",
      " ...\n",
      " [0.991868   0.00813198]\n",
      " [0.9925789  0.00742109]\n",
      " [0.9662503  0.0337497 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = predictions()\n",
    "\n",
    "df = pred.normalize(df_transaction)\n",
    "\n",
    "df = pred.encoding(df)\n",
    "test = pred.encoding(df_transaction_test)\n",
    "\n",
    "df = df.drop([\"isFraud\"],axis=1)\n",
    "\n",
    "#df = pred.feature_selection(df,100)\n",
    "df[\"isFraud\"] = df_transaction.isFraud\n",
    "\n",
    "\n",
    "auc, fpr,tpr,therashold, report, accuracy, con_mat, final_result = pred.boosting(df.iloc[:,:-1],df.isFraud,df_transaction_test)\n",
    "print(\"area under the curve{}\".format(auc))\n",
    "print(\"total accuracy{}\".format(accuracy))\n",
    "print(\"confusion matrix is \\n {} \".format(con_mat))\n",
    "print(report)\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\")\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_predictions = pd.read_csv(\"sample_submission.csv\")\n",
    "final_test_predictions.isFraud = 1 - final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>0.060295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>0.011488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>0.021932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>0.034831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>0.012847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID   isFraud\n",
       "0        3663549  0.060295\n",
       "1        3663550  0.011488\n",
       "2        3663551  0.021932\n",
       "3        3663552  0.034831\n",
       "4        3663553  0.012847"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461266\n",
      "45425\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x = df_transaction.iloc[:,17:30]\n",
    "y = df_transaction.isFraud\n",
    "y= pd.get_dummies(y).values\n",
    "train_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.25, random_state=28)\n",
    "\n",
    "numb_features = train_x.shape[1]\n",
    "numb_class = train_y.shape[1]\n",
    "\n",
    "X= tf.placeholder(tf.float32,[None,numb_features])\n",
    "Y_class = tf.placeholder(tf.float32,[None,numb_class])\n",
    "\n",
    "weights = tf.Variable(tf.zeros([numb_features,numb_class]))\n",
    "bais = tf.Variable(tf.zeros([1,numb_class]))\n",
    "\n",
    "matrix_multiplication = tf.matmul(X,weights)\n",
    "addition = tf.add(matrix_multiplication,bais)\n",
    "result  = tf.nn.sigmoid(addition)\n",
    "\n",
    "epocs = 500\n",
    "\n",
    "#learning_rate = tf.train.exponential_decay(learning_rate=0.1)\n",
    "cost = tf.nn.l2_loss(result - Y_class)\n",
    "\n",
    "training = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(result,1),tf.argmax(train_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "acc_values = []\n",
    "cost_values = []\n",
    "epoc = []\n",
    "old_cost = 0\n",
    "\n",
    "for i in range(epocs):\n",
    "    sess.run(training,feed_dict={X:train_x,Y_class:train_y})\n",
    "\n",
    "    if i%10==0:\n",
    "        epoc.append(i)\n",
    "        accu , costs = sess.run([accuracy,cost],feed_dict={X:train_x,Y_class:train_y}) \n",
    "\n",
    "        acc_values.append(accu)\n",
    "        cost_values.append(costs)\n",
    "\n",
    "        diff = abs(costs-old_cost)\n",
    "        old_cost = costs\n",
    "\n",
    "        print(\"step number {}, accuracy is {}, cost is {}, change in cost {}\".format(i,accu,costs,diff))\n",
    "\n",
    "#roc_curve(result, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_predictions.to_csv(\"sub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under the curve = 0.8112574417333316\n",
      "total accuracy = 0.8110723964382501\n",
      "confusion matrix is \n",
      " [[4351  788]\n",
      " [1164 4029]]  \n"
     ]
    }
   ],
   "source": [
    "print(\"area under the curve = {}\".format(auc))\n",
    "print(\"total accuracy = {}\".format(accuracy))\n",
    "print(\"confusion matrix is \\n {}  \".format(con_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "obj = [columns for columns in train.columns if train[columns].dtypes==\"object\"]\n",
    "le = LabelEncoder()\n",
    "for col in obj:\n",
    "    train[col] = le.fit_transform(train[col].astype(\"str\").values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions()\n",
    "df = pred.feature_selection(v_only.fillna(0),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = SelectKBest(chi2,k=100)\n",
    "fit = best.fit_transform(v_only.iloc[:,:-1].fillna(0),v_only.iloc[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(learning_rate=learning_rate)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions\n",
    "\n",
    "accuracy =  predictions.logistic(try1)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross check fraud and productCD\n",
    "\n",
    "pd.crosstab(df_transaction.isFraud,df_transaction.ProductCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is difference between dimension reduction(PCA) and feature selection(RIDGE LASSO)\n",
    "# FEATURE SELECTION SHOULD BE PERFORMED IN EACH LOOP OF CROSS VALIDATION. \n",
    "#Do not perform feature selction only once and then use cross-validation, this will elad to overfitting.\n",
    "# correlation calclatino can also be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"kshitiz\",\"sirohi\",\" itraz\"]\n",
    "b = \"it\"\n",
    "for i in a:\n",
    "    print(b in i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= \"C\"\n",
    "columns_c = df_transaction.iloc[:,[True for i in df_transaction.columns if c in i ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try2 = df_transaction.iloc[:,10:]\n",
    "try2[\"isFraud\"] = df_transaction.isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksiro\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c_only = try2.filter(like=\"C\")\n",
    "c_only[\"isFraud\"]= try2.isFraud\n",
    "\n",
    "d_only = try2.filter(like=\"D\")\n",
    "d_only[\"isFraud\"] = try2.isFraud\n",
    "d_only = d_only.fillna(0)\n",
    "\n",
    "m_only = try2.filter(like=\"M\")\n",
    "m_only[\"isFraud\"] = try2.isFraud\n",
    "m_only = m_only.fillna(0)\n",
    "\n",
    "v_only = try2.filter(like=\"V\")\n",
    "v_only[\"isFraud\"] =try2.isFraud\n",
    "v_only = v_only.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C1   C2   C3   C4   C5   C6   C7   C8   C9  C10  C11  C12   C13  C14  \\\n",
       "0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  2.0  0.0   1.0  1.0   \n",
       "1  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   1.0  1.0   \n",
       "2  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0   1.0  1.0   \n",
       "3  2.0  5.0  0.0  0.0  0.0  4.0  0.0  0.0  1.0  0.0  1.0  0.0  25.0  1.0   \n",
       "4  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0   1.0  1.0   \n",
       "\n",
       "   isFraud  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_only = c_only.drop(\"ProductCD\",axis=1)\n",
    "c_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx, testx, trainy, testy = train_test_split(v_only.iloc[:,:-1],v_only.isFraud,test_size=.3)\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model,100)\n",
    "#rfe = rfe.fit(trainx,trainy)\n",
    "print(rfe.support_)\n",
    "\n",
    "#model.fit(trainx,trainy)\n",
    "#pred = model.predict(testx)\n",
    "#acc = accuracy_score(pred,testy)\n",
    "#acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(testy,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "best = SelectKBest(chi2,4)\n",
    "fit = best.fit(c_only.iloc[:,:-1],c_only.iloc[:,-1])\n",
    "score = pd.DataFrame(fit.scores_)\n",
    "cols = pd.DataFrame(c_only.columns)\n",
    "fs = pd.concat([score,cols],axis=1)\n",
    "fs.columns = [\"score\",\"col\"]\n",
    "fs = fs.sort_values(\"score\",ascending=False)\n",
    "top100 = fs.head(30).col\n",
    "c_only = c_only.loc[:,top100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_only[\"isFraud\"]= df_transaction.isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True,formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x , train_y,test_y = train_test_split(c_only.iloc[:,:-1],c_only.isFraud)\n",
    "smt = SMOTE()\n",
    "train_x,train_y = smt.fit_sample(train_x,train_y)\n",
    "#model = xgb.XGBClassifier()\n",
    "#model.fit(train_x,train_y)\n",
    "#pred_prob = model.predict_proba(test_x)\n",
    "#pred_prob = pred_prob[:,1]\n",
    "#pred = model.predict(test_x)\n",
    "#acc = accuracy_score(test_y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame(train_x)\n",
    "train_x.columns= test_x.columns\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x,test_x , train_y,test_y = train_test_split(train.iloc[:,:-1],train.isFraud)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(train.iloc[:,15:29], train.isFraud)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "best = SelectKBest(chi2,100)\n",
    "fit = best.fit_transform(train.iloc[:,:-1],train.iloc[:,-1])\n",
    "fit = pd.DataFrame(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictions()\n",
    "x,y = pred.normalize(df_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame({\"a\":[4,5,7,-3],\"b\":[\"a\",\"g\",\"g\",\"t\"]})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame({\"a\":[44,52,43,23],\"b\":[\"aw\",\"ge\",\"gg\",\"tr\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.concat([d,p])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[c < 0 ]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': [0, -1, 2], 'b': [-3, 2, 1],\n",
    "                           'c': ['foo', 'goo', 'bar']})\n",
    "df[df < 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df < 0].sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class trying(object):\n",
    "    \n",
    "    def first(self,a,b):\n",
    "        self.c = a+b+ self.second(5,5)\n",
    "        return self.c\n",
    "    \n",
    "    def second(self,a,b):\n",
    "        return a+b\n",
    "    \n",
    "t = trying()\n",
    "t.first(1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
